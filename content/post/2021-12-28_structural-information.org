---
title: "结构信息论"
date: 2021-12-28T15:08:33+08:00
draft: false
tags: ['information']
categories: ['learning']
math: true
---

给定一个分布，想要描述一个服从该分布的随机变量取值平均所需的信息量就是该分布的熵，也可以理解为该分布的不确定性。信息熵越大，该分布的不确定性越大，完全确定取值所需的信息就越多。例如在一个随机变量有8种等可能取值的分布中，为了确定一个结果，需要一个能够容纳8个不同值的标识。因此用3bit字符串足够描述这些标识。如果分布是非均匀的，则可以对概率大的取值用较短的描述，这样我们能得到一个平均更短的描述。这个 *描述的平均长度* 就是该分布的信息熵。我们定义某一分布的信息熵如下：

[[/img/2021-12-28_entropy.png]]

直观地解释， $l=-logp_i$ 是描述概率为 $p_i$ 的事件的二进制长度，熵就是每个事件的描述长度的加权平均。

* 位置熵/一维结构熵
我们把一张图 $G$ 的度分布的熵叫做图 $G$ 的位置熵，也是图的一维结构熵。

$$
\begin{aligned}
\mathcal{H}^{1}(G) &=H(\mathbf{p})=H\left(\frac{d_{1}}{2 m}, \ldots, \frac{d_{n}}{2 m}\right) \\
&=-\sum_{i=1}^{n} \frac{d_{i}}{2 m} \cdot \log _{2} \frac{d_{i}}{2 m}
\end{aligned}
$$

其中 $m$ 为图 $G = (V, E)$ 中边的数量，即 $|E|$ ； $d_i$ 为结点 $i$ 的度。一维结构熵 $H^i(G)$ ，从直观上解释，是确定在有静态分布的图 $G$ 中随机游走能够达到的结点的一维编码所需的信息量。

现在让我们进一步理解上面这个公式。我们之前对的香农熵解释是：对分布中的每个取值，根据该取值出现的概率给他一个码字（编码）长度，使得描述分布中取值所需的平均编码长度最小。这个最小的编码长度就是分布的信息熵。在位置熵中，我们可以认为简单图中一个结点在随机游走中被访问的概率就是该结点的度在所有结点的度之和中所占的比例。因此若要给该结点一个编码，最优的编码长度应当是 $-\log_2\frac{d_i}{2m}$ 。加权平均后记为该图 $G$ 的位置熵/一维结构熵 $H^1(G)$ 。

* 高维结构熵
* 结构熵最小化
